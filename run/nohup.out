W0418 23:15:58.193576 140252409528960 torch/distributed/run.py:779] 
W0418 23:15:58.193576 140252409528960 torch/distributed/run.py:779] *****************************************
W0418 23:15:58.193576 140252409528960 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0418 23:15:58.193576 140252409528960 torch/distributed/run.py:779] *****************************************
[2025-04-18 23:16:01,775] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-18 23:16:01,837] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-18 23:16:03,929] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-04-18 23:16:03,956] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-04-18 23:16:03,956] [INFO] [comm.py:689:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:03<00:07,  3.62s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:03<00:07,  3.64s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:07<00:03,  3.63s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:07<00:03,  3.62s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:10<00:00,  3.49s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:10<00:00,  3.52s/it]
trainable params: 20,971,520 || all params: 7,262,703,616 || trainable%: 0.2888
Tokenizing train dataset:   0%|          | 0/1942 [00:00<?, ? examples/s]Loading checkpoint shards: 100%|██████████| 3/3 [00:10<00:00,  3.47s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:10<00:00,  3.51s/it]
trainable params: 20,971,520 || all params: 7,262,703,616 || trainable%: 0.2888
Tokenizing train dataset:  51%|█████▏    | 1000/1942 [00:07<00:06, 141.99 examples/s]Tokenizing train dataset: 100%|██████████| 1942/1942 [00:14<00:00, 134.51 examples/s]Tokenizing train dataset: 100%|██████████| 1942/1942 [00:14<00:00, 135.12 examples/s]
Tokenizing eval dataset:   0%|          | 0/216 [00:00<?, ? examples/s]Tokenizing eval dataset: 100%|██████████| 216/216 [00:01<00:00, 139.96 examples/s]Tokenizing eval dataset: 100%|██████████| 216/216 [00:01<00:00, 138.86 examples/s]
Tokenizing train dataset:   0%|          | 0/1942 [00:00<?, ? examples/s][rank0]: Traceback (most recent call last):
[rank0]:   File "../src/dpo_train.py", line 175, in <module>
[rank0]:     main()
[rank0]:   File "../src/dpo_train.py", line 164, in main
[rank0]:     dpo_train(
[rank0]:   File "../src/dpo_train.py", line 127, in dpo_train
[rank0]:     trainer.train()
[rank0]:   File "/home/yaojinyu/miniconda3/envs/pytorch/lib/python3.8/site-packages/transformers/trainer.py", line 1948, in train
[rank0]:     return inner_training_loop(
[rank0]:   File "/home/yaojinyu/miniconda3/envs/pytorch/lib/python3.8/site-packages/transformers/trainer.py", line 2046, in _inner_training_loop
[rank0]:     self.optimizer, self.lr_scheduler = deepspeed_init(self, num_training_steps=max_steps)
[rank0]:   File "/home/yaojinyu/miniconda3/envs/pytorch/lib/python3.8/site-packages/transformers/integrations/deepspeed.py", line 393, in deepspeed_init
[rank0]:     hf_deepspeed_config.trainer_config_finalize(args, model, num_training_steps)
[rank0]:   File "/home/yaojinyu/miniconda3/envs/pytorch/lib/python3.8/site-packages/transformers/integrations/deepspeed.py", line 265, in trainer_config_finalize
[rank0]:     raise ValueError(
[rank0]: ValueError: Please correct the following DeepSpeed config values that mismatch TrainingArguments values:
[rank0]: - ds optimizer.params.lr=1e-05 vs hf learning_rate=5e-06
[rank0]: The easiest method is to set these DeepSpeed config values to 'auto'.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
W0418 23:16:36.864331 140252409528960 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 430566 closing signal SIGTERM
E0418 23:16:37.681717 140252409528960 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 0 (pid: 430565) of binary: /home/yaojinyu/miniconda3/envs/pytorch/bin/python
Traceback (most recent call last):
  File "/home/yaojinyu/miniconda3/envs/pytorch/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/yaojinyu/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/home/yaojinyu/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/home/yaojinyu/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/yaojinyu/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/yaojinyu/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
../src/dpo_train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-04-18_23:16:36
  host      : ps
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 430565)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================

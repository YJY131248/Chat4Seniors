W0420 00:19:43.271193 140706434925184 torch/distributed/run.py:779] 
W0420 00:19:43.271193 140706434925184 torch/distributed/run.py:779] *****************************************
W0420 00:19:43.271193 140706434925184 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0420 00:19:43.271193 140706434925184 torch/distributed/run.py:779] *****************************************
[2025-04-20 00:19:46,798] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-20 00:19:46,857] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-20 00:19:48,934] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-04-20 00:19:48,936] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-04-20 00:19:48,936] [INFO] [comm.py:689:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:03<00:07,  3.56s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:03<00:07,  3.57s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:07<00:03,  3.58s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:07<00:03,  3.59s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:10<00:00,  3.44s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:10<00:00,  3.48s/it]
trainable params: 20,971,520 || all params: 7,262,703,616 || trainable%: 0.2888
Tokenizing train dataset:   0%|          | 0/1942 [00:00<?, ? examples/s]Loading checkpoint shards: 100%|██████████| 3/3 [00:10<00:00,  3.43s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:10<00:00,  3.47s/it]
trainable params: 20,971,520 || all params: 7,262,703,616 || trainable%: 0.2888
Tokenizing train dataset:  51%|█████▏    | 1000/1942 [00:07<00:06, 138.85 examples/s]Tokenizing train dataset: 100%|██████████| 1942/1942 [00:14<00:00, 135.56 examples/s]Tokenizing train dataset: 100%|██████████| 1942/1942 [00:14<00:00, 135.57 examples/s]
Tokenizing eval dataset:   0%|          | 0/216 [00:00<?, ? examples/s]Tokenizing eval dataset: 100%|██████████| 216/216 [00:01<00:00, 130.67 examples/s]Tokenizing eval dataset: 100%|██████████| 216/216 [00:01<00:00, 129.72 examples/s]
Tokenizing train dataset:   0%|          | 0/1942 [00:00<?, ? examples/s]Tokenizing train dataset:  51%|█████▏    | 1000/1942 [00:07<00:06, 134.79 examples/s]Tokenizing train dataset: 100%|██████████| 1942/1942 [00:14<00:00, 133.15 examples/s]Tokenizing train dataset: 100%|██████████| 1942/1942 [00:14<00:00, 132.92 examples/s]
Tokenizing eval dataset:   0%|          | 0/216 [00:00<?, ? examples/s]Tokenizing eval dataset: 100%|██████████| 216/216 [00:01<00:00, 133.21 examples/s]Tokenizing eval dataset: 100%|██████████| 216/216 [00:01<00:00, 132.21 examples/s]
Using /home/yaojinyu/.cache/torch_extensions/py38_cu121 as PyTorch extensions root...
Using /home/yaojinyu/.cache/torch_extensions/py38_cu121 as PyTorch extensions root...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yaojinyu/.cache/torch_extensions/py38_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.7300686836242676 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.8041872978210449 seconds
  0%|          | 0/4855 [00:00<?, ?it/s]Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
  0%|          | 1/4855 [00:02<3:32:15,  2.62s/it]  0%|          | 2/4855 [00:04<2:39:25,  1.97s/it]  0%|          | 3/4855 [00:05<2:18:26,  1.71s/it]  0%|          | 4/4855 [00:09<3:18:28,  2.45s/it]  0%|          | 5/4855 [00:11<3:13:55,  2.40s/it]  0%|          | 6/4855 [00:13<3:04:06,  2.28s/it]  0%|          | 7/4855 [00:16<3:34:36,  2.66s/it]  0%|          | 8/4855 [00:18<3:04:41,  2.29s/it]  0%|          | 9/4855 [00:22<3:47:24,  2.82s/it]  0%|          | 10/4855 [00:24<3:36:10,  2.68s/it]  0%|          | 11/4855 [00:27<3:31:39,  2.62s/it]  0%|          | 12/4855 [00:28<2:53:38,  2.15s/it]  0%|          | 13/4855 [00:29<2:39:33,  1.98s/it]  0%|          | 14/4855 [00:32<2:58:31,  2.21s/it]  0%|          | 15/4855 [00:34<2:54:24,  2.16s/it]  0%|          | 16/4855 [00:37<3:02:05,  2.26s/it]  0%|          | 17/4855 [00:39<2:54:46,  2.17s/it]  0%|          | 18/4855 [00:40<2:39:01,  1.97s/it]  0%|          | 19/4855 [00:44<3:12:27,  2.39s/it]  0%|          | 20/4855 [00:44<2:38:04,  1.96s/it]  0%|          | 21/4855 [00:47<2:45:04,  2.05s/it]  0%|          | 22/4855 [00:51<3:32:21,  2.64s/it]
W0418 01:03:10.272037 140296725623424 torch/distributed/run.py:779] 
W0418 01:03:10.272037 140296725623424 torch/distributed/run.py:779] *****************************************
W0418 01:03:10.272037 140296725623424 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0418 01:03:10.272037 140296725623424 torch/distributed/run.py:779] *****************************************
[2025-04-18 01:03:13,746] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-18 01:03:13,759] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-18 01:03:15,730] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-04-18 01:03:15,786] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-04-18 01:03:15,786] [INFO] [comm.py:689:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:03<00:07,  3.51s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:03<00:07,  3.50s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:07<00:03,  3.51s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:07<00:03,  3.50s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:10<00:00,  3.37s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:10<00:00,  3.41s/it]
trainable params: 20,971,520 || all params: 7,262,703,616 || trainable%: 0.2888
Tokenizing train dataset:   0%|          | 0/1942 [00:00<?, ? examples/s]Loading checkpoint shards: 100%|██████████| 3/3 [00:10<00:00,  3.36s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:10<00:00,  3.40s/it]
trainable params: 20,971,520 || all params: 7,262,703,616 || trainable%: 0.2888
Tokenizing train dataset:  51%|█████▏    | 1000/1942 [00:06<00:06, 143.15 examples/s]Tokenizing train dataset: 100%|██████████| 1942/1942 [00:14<00:00, 137.74 examples/s]Tokenizing train dataset: 100%|██████████| 1942/1942 [00:14<00:00, 138.10 examples/s]
Tokenizing eval dataset:   0%|          | 0/216 [00:00<?, ? examples/s]Tokenizing eval dataset: 100%|██████████| 216/216 [00:01<00:00, 133.65 examples/s]Tokenizing eval dataset: 100%|██████████| 216/216 [00:01<00:00, 132.73 examples/s]
Tokenizing train dataset:   0%|          | 0/1942 [00:00<?, ? examples/s]Tokenizing train dataset:  51%|█████▏    | 1000/1942 [00:07<00:07, 134.43 examples/s]Tokenizing train dataset: 100%|██████████| 1942/1942 [00:14<00:00, 135.22 examples/s]Tokenizing train dataset: 100%|██████████| 1942/1942 [00:14<00:00, 134.66 examples/s]
Tokenizing eval dataset:   0%|          | 0/216 [00:00<?, ? examples/s]Tokenizing eval dataset: 100%|██████████| 216/216 [00:01<00:00, 135.03 examples/s]Tokenizing eval dataset: 100%|██████████| 216/216 [00:01<00:00, 134.09 examples/s]
Using /home/yaojinyu/.cache/torch_extensions/py38_cu121 as PyTorch extensions root...
Using /home/yaojinyu/.cache/torch_extensions/py38_cu121 as PyTorch extensions root...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Detected CUDA files, patching ldflags
Emitting ninja build file /home/yaojinyu/.cache/torch_extensions/py38_cu121/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.4964478015899658 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.5028038024902344 seconds
  0%|          | 0/2913 [00:00<?, ?it/s]Could not estimate the number of tokens of the input, floating-point operations will not be computed
Could not estimate the number of tokens of the input, floating-point operations will not be computed
  0%|          | 1/2913 [00:02<1:51:00,  2.29s/it]  0%|          | 2/2913 [00:03<1:17:04,  1.59s/it]  0%|          | 3/2913 [00:06<1:41:18,  2.09s/it]  0%|          | 4/2913 [00:07<1:22:16,  1.70s/it]  0%|          | 5/2913 [00:09<1:39:20,  2.05s/it]  0%|          | 6/2913 [00:12<1:47:37,  2.22s/it]  0%|          | 7/2913 [00:14<1:40:08,  2.07s/it]  0%|          | 8/2913 [00:18<2:09:44,  2.68s/it]  0%|          | 9/2913 [00:20<2:03:34,  2.55s/it]  0%|          | 10/2913 [00:24<2:25:27,  3.01s/it]  0%|          | 11/2913 [00:26<2:14:08,  2.77s/it]  0%|          | 12/2913 [00:28<1:53:53,  2.36s/it]  0%|          | 13/2913 [00:31<2:03:43,  2.56s/it]  0%|          | 14/2913 [00:34<2:18:18,  2.86s/it]  1%|          | 15/2913 [00:37<2:23:53,  2.98s/it]  1%|          | 16/2913 [00:40<2:17:01,  2.84s/it]  1%|          | 17/2913 [00:42<2:08:10,  2.66s/it]  1%|          | 18/2913 [00:44<2:01:25,  2.52s/it]  1%|          | 19/2913 [00:46<1:52:23,  2.33s/it]  1%|          | 20/2913 [00:48<1:51:09,  2.31s/it]  1%|          | 21/2913 [00:50<1:43:26,  2.15s/it]  1%|          | 22/2913 [00:52<1:41:27,  2.11s/it]  1%|          | 23/2913 [00:56<2:03:09,  2.56s/it]  1%|          | 24/2913 [00:59<2:09:21,  2.69s/it]